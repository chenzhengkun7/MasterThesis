\chapter{Introduction au Réseaux de Neurones Artificiels}
\chapterintrobox{Les réseaux neuronaux artificiels sont des systèmes de computation capables de trouver des fonctions complexes qui relient une entrée à une sortie. ces systèmes peuvent être utilisés pour une variété de tâches comme la régression et la classification. Ils peuvent être utilisés dans le domaine de la maintenance prédictive et des pronostics pour estimer l'état de santé de l'équipement et prévoir avec une certaine incertitude sa durée de vie utile restante. Ce chapitre traite les réseaux de neurones, leur topologie, leur entraînement et leur formulation mathématique.}


\section{La structure de réseaux de neurones artificiels}
Les réseaux de neurones artificiels sont des systèmes de computation utilisés pour trouver la correspondance entre une entrée et une sortie, ils sont constitués de plusieurs couches (couche d'entrée, couche de sortie et un nombre arbitraire de couches cachées entre l'entrée et la sortie) et chaque couche contient un certain nombre de neurones où chaque neurone de chaque couche est connecté à tous les neurones de la couche précédente et de la couche suivante (à l'exception des couches d'entrée et de sortie qui sont connectées uniquement aux couches suivante et précédente respectivement).
chaque neurone de chaque couche reçoit une entrée des neurones de la couche précédente (sous la forme d'un vecteur), multiplie le vecteur par quelques poids et somme le résultat puis applique une fonction d'activation linéaire. Chaque neurone se retrouve avec une seule valeur numérique appelée activation, qui sera transmise aux neurones de la couche suivante.

Figure \ref{fig:neural-network-structure} montre un réseau de neurones avec la structure suivante :

\begin{itemize}
    \item Une couche d'entrée avec 3 entrée
    \item Une seule couche cachée avec 4 neurones
    \item Une couche de sortie avec 1 neurone 
\end{itemize}

\begin{figure}[h]
    \centering
	\input{figures/mlp-structure.tex}
    \caption{Structure d'un réseau de neurones}
    \label{fig:neural-network-structure}
\end{figure}

\section{Feedforward: de l'entrée vers la sortie}
L'architecture de la figure \ref{fig:neural-network-structure} est appelée Feedforward Neural Network, c'est une architecture acyclique où l'information circule de la première à la dernière couche sans aucune boucle interne, contrairement aux autres architectures comme les réseaux de neurones récurrents. La première couche est la couche d'entrée, elle n'effectue aucune opération et se limite à recevoir l'entrée. L'entrée est un vecteur de nombres représente les différentes variables. Le vecteur est multiplié par une matrice de poids qui le transforme et l'envoie à la couche suivante (ou à la première couche cachée). Une fonction d'activation est appliquée aux valeurs résultant de la multiplication des valeurs de la couche précédente avec la matrice des poids, le résultat devient les valeurs de la couche suivante, ou activations (la valeur de chaque neurone est appelée activaiton).

Une formule générale pour passer de la couche $l-1$ à la couche $l$ est donnée par l'équation \ref{equation:forward-step}:

\begin{equation}
    a^{[l]} = g^{[l]}(W^{[l]}a^{[l-1]}+b^{[l]})
    \label{equation:forward-step}
\end{equation}

Où $g^{[l]}$ est la fonction d'activation de la couche $l$, $W^{[l]}$ est la matrice des poids qui transforme les valeurs (activations) de la couche $l-1$ à la couche $l$ et $a^{[l]}$ représente les activations de la couche $l$. $b$ est la valeur du biais (ou valeur d'interception), elle est ajoutée à la multiplication entre les activations et la matrice des poids, c'est un paramètre qui peut être appris comme les poids. L'opération se répète pour chaque couche jusqu'à la couche de sortie.
La première couche peut être considérée comme la couche 0 et les entrées peuvent être désignées par le vecteur $a^{[0]}$.

\section{Fonction d'activation}
Les fonctions d'activation sont appliquées au résultat de la multiplication des entrées de la couche précédente avec les poids correspondants, pour déterminer la valeur de chaque neurone. Il existe différents types de ces fonctions.

\subsection{L'importance des fonctions d'activation non linéaires}
L'utilisation de la fonction d'activation non linéaire est très importante pour les réseaux de neurones, ils permettent d'apprendre la correspondance non linéaire complexe de l'entrée à la sortie. Si le réseau n'utilise pas l'activation non linéaire (par exemple, l'activation linéaire ou la fonction d'identité), alors le réseau entier (quelle que soit sa profondeur) est équivalent à un réseau avec une seule couche cachée.

\subsection{Types de fonctions d'activation}
Il existe une variété de fonctions d'activation qui peuvent être utilisées pour les couches cachées et de sortie. La figure \ref{fig:activation-function} montre quelques examples:

\begin{figure}[h]
    \centering
	\input{figures/activation-functions.tex}
    \caption{Différentes fonctions d'activation}
    \label{fig:activation-function}
\end{figure}

Tablea \ref{table:activation-functions} montre les définition mathematiques de quelques fonctions d'activation:

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        \hline
        Fonction d'activation & Défnition mathématique \\
        \hline
        Identity (pas d'activation) & Id(x) = x \\
        Sigmoid & $\sigma(x)= \frac{1}{1+e^{-x}}$ \\
        tanh & $tanh(x)=\frac{(e^x-e^{-x})}{(e^x+e^{-x})}$\\
        Rectified Linear Unit (ReLU) & $ReLU(x)=max(0,x)$\\
    Leaky ReLU & $LeakyReLU(x)=max(0.1 x,x)$
        
    \end{tabular}
    \caption{Définitions mathématiques de quelques fonctions d'activation}
    \label{table:activation-functions}
\end{table}

\section{Entraînement du réseau}
Le processus de formation d'un réseau de neurones consiste à déterminer les poids (coefficients) qui relient les neurones de chaque couche aux neurones de la couche suivante. Ce processus peut être formulé en termes plus mathématiques comme un problème d'optimisation : optimisation des coefficients du réseau pour trouver leurs valeurs qui minimisent une fonction de coût.

L'entraînement du réseau de neurones utilise des données d'entraînement (training data) qui fournissent des entrées et leurs sorties correspondantes.

\subsection{Fonction de coût}
La fonction de coût est la fonction utilisée pour calculer la différence entre la sortie du réseau de neurones et la sortie réelle attendue, elle quantifie la performance du réseau. Le but du processus d'entraînement est de minimiser cette fonction en utilisant Gradient Descent (voir la section suivante) pour trouver le meilleur ensemble de poids qui donne la différence la plus basse entre les données d'entraînement et la prédiction du réseau.

\subsection{Gradient Descent}
Gradient Descent est un algorithme d'optimisation itératif utilisé pour optimiser une fonction différentiable. Gradient Descent fonctionne en calculant les gradients de la fonction objectif puis en prenant des mesures itératives dans le sens du négatif des gradients.

\subsection{Backpropagation}
Backpropagation est l'algorithme utilisé pour calculer les gradients de la fonction de coût (la fonction objectif) d'un réseau de neurones. Comme un réseau de neurones peut être interprété comme une fonction composite, Backpropagation utilise le théorème de dérivation des fonctions composées
pour trouver les gradients par rapport aux poids du réseau.

L'utilisation de Backpropagation pour l'entraînement de réseaux de neurones a été popularisée par David E. Rumelhart, Geoffrey E. Hintont et Ronald J. Williams \cite{Rumelhart1986}, ils l'ont décrit comme une procédure qui ajuste de manière répétée les poids des connexions du réseau de manière à minimiser une mesure de la différence entre le vecteur de sortie réel du réseau et le vecteur de sortie souhaité. À la suite des ajustements de poids, des unités internes "cachées" qui ne font pas partie de l'entrée ou de la sortie en viennent à représenter des caractéristiques importantes du domaine de la tâche, et les régularités de la tâche sont saisies par les interactions de ces unités.

Figure \ref{fig:forward-backward-pass} représente le Forward Pass et le Backward Pass, le Forward Pass calcule la sortie de réseau, le Backward pass calcule les gradients de la fonction de coût qui mesure la difference entre cette sortie et la sortie réelle souhaitée:

\begin{figure}[h]
    \centering
	\input{figures/forward-backward-pass.tex}
    \caption{Forward et Backward passes}
    \label{fig:forward-backward-pass}
\end{figure}


\section{Réseaux de neurones récurrents}

\section{Conclusion}
Les réseaux de neurones sont un outil puissant pour trouver la relation complexe entre une entrée et une sortie (e.g. les données de \acrlong{cm} et la dégradation de la machine). Ils sont constitués de différentes couches de neurones interconnectées. la connexion entre chaque 2 couches est définie par un ensemble de poids, qui sont multipliés par les valeurs de la couche précédente pour trouver les valeurs de la couche suivante, le processus est répété jusqu'à ce que la couche de sortie soit atteinte. la sortie prédite est comparée à la sortie réelle obtenue à partir des données d'entraînement, les poids sont ajustés pour minimiser (en utilisant Backpropagation et Gradient Descent) la différence entre les prédictions et la réalité.

\begin{comment}
    \begin{tikzpicture}
    \begin{axis}[
    hide axis,
    xlabel=$x$,ylabel=$y$,
    mesh/interior colormap name=hot,
    colormap/blackwhite,
    ]
    \addplot3 [domain=-1.5:1.5,surf]
    {-exp(-x^2-y^2)};
    \end{axis}
    \end{tikzpicture}

    \begin{tikzpicture}
        \begin{axis}[
        hide axis,
        xlabel=$x$,
        ylabel=$y$,
        ]
        \addplot3 [
        surf,
        mesh/interior colormap={blueblack}{
        color=(black) color=(blue)
        },
        % slightly increase sampling quality (was 25):
        samples=31,
        % avoids overshooting corners:
        miter limit=1,
        % move boundary between inner and outer:
        mesh/interior colormap thresh=0.1,
        colormap/blackwhite,
        domain=0:1,
        ] {
        sin(deg(8*pi*x))* exp(-20*(y-0.5)^2)
        + exp(-(x-0.5)^2*30
        - (y-0.25)^2 - (x-0.5)*(y-0.25))
        };
        \end{axis}
        \end{tikzpicture}
\end{comment}